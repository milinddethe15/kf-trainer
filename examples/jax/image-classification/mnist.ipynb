{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Distributed MNIST with JAX and Kubeflow Trainer\n",
    "\n",
    "This notebook demonstrates how to run distributed JAX training on Kubernetes using the Kubeflow Trainer SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install the Kubeflow SDK\n",
    "\n",
    "You need to install the Kubeflow SDK to interact with Kubeflow Trainer APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kubeflow in /opt/miniconda3/lib/python3.12/site-packages (0.3.0)\n",
      "Requirement already satisfied: kubeflow-katib-api>=0.19.0 in /opt/miniconda3/lib/python3.12/site-packages (from kubeflow) (0.19.0)\n",
      "Requirement already satisfied: kubeflow-trainer-api>=2.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from kubeflow) (2.1.0)\n",
      "Requirement already satisfied: kubernetes>=27.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from kubeflow) (35.0.0)\n",
      "Requirement already satisfied: pydantic>=2.10.0 in /opt/miniconda3/lib/python3.12/site-packages (from kubeflow) (2.12.5)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/miniconda3/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/miniconda3/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/miniconda3/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/miniconda3/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow) (6.0.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/miniconda3/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow) (1.9.0)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/miniconda3/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow) (2.0.0)\n",
      "Requirement already satisfied: urllib3!=2.6.0,>=1.24.2 in /opt/miniconda3/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/miniconda3/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow) (0.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow) (0.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests->kubernetes>=27.2.0->kubeflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests->kubernetes>=27.2.0->kubeflow) (3.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from requests-oauthlib->kubernetes>=27.2.0->kubeflow) (3.3.1)\n"
     ]
    }
   ],
   "source": [
    " #!pip install -U kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Function\n",
    "\n",
    "This function will be serialized and executed on each JAX worker node. It uses:\n",
    "- **Flax NNX** for model definition\n",
    "- **Optax** for optimization (functional API)\n",
    "- **`@jax.jit`** for XLA compilation\n",
    "- **Data parallelism** across nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:19:46.917723Z",
     "iopub.status.busy": "2025-09-03T13:19:46.917308Z",
     "iopub.status.idle": "2025-09-03T13:19:46.935181Z",
     "shell.execute_reply": "2025-09-03T13:19:46.934697Z",
     "shell.execute_reply.started": "2025-09-03T13:19:46.917698Z"
    }
   },
   "outputs": [],
   "source": [
    "def jax_train_mnist():\n",
    "    import os\n",
    "    import time\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    import numpy as np\n",
    "    from flax import nnx\n",
    "    import optax\n",
    "\n",
    "    # Initialize JAX distributed using environment variables set by the runtime.\n",
    "    coordinator_address = os.environ.get(\"JAX_COORDINATOR_ADDRESS\")\n",
    "    num_processes = int(os.environ.get(\"JAX_NUM_PROCESSES\", 1))\n",
    "    process_id = int(os.environ.get(\"JAX_PROCESS_ID\", 0))\n",
    "\n",
    "    jax.distributed.initialize(\n",
    "        coordinator_address=coordinator_address,\n",
    "        num_processes=num_processes,\n",
    "        process_id=process_id,\n",
    "    )\n",
    "\n",
    "    print(f\"JAX distributed initialized: Process {process_id}/{num_processes}\")\n",
    "    print(f\"Available devices: {jax.devices()}\")\n",
    "\n",
    "    # Define simple MLP model with Flax NNX.\n",
    "    class MLP(nnx.Module):\n",
    "        def __init__(self, in_dims: int, hidden_dims: int, out_dims: int, *, rngs: nnx.Rngs):\n",
    "            self.linear1 = nnx.Linear(in_dims, hidden_dims, rngs=rngs)\n",
    "            self.linear2 = nnx.Linear(hidden_dims, hidden_dims, rngs=rngs)\n",
    "            self.linear3 = nnx.Linear(hidden_dims, out_dims, rngs=rngs)\n",
    "\n",
    "        def __call__(self, x):\n",
    "            x = nnx.relu(self.linear1(x))\n",
    "            x = nnx.relu(self.linear2(x))\n",
    "            return self.linear3(x)\n",
    "\n",
    "    # Download MNIST dataset.\n",
    "    def load_mnist():\n",
    "        import urllib.request\n",
    "        import gzip\n",
    "\n",
    "        def download(url, filename):\n",
    "            filepath = f\"/tmp/{filename}\"\n",
    "            if not os.path.exists(filepath):\n",
    "                urllib.request.urlretrieve(url, filepath)\n",
    "            with gzip.open(filepath, \"rb\") as f:\n",
    "                data = np.frombuffer(f.read(), np.uint8, offset=16 if \"images\" in filename else 8)\n",
    "            return data\n",
    "\n",
    "        base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "        train_images = download(base_url + \"train-images-idx3-ubyte.gz\", \"train-images\").reshape(-1, 784)\n",
    "        train_labels = download(base_url + \"train-labels-idx1-ubyte.gz\", \"train-labels\")\n",
    "        return train_images.astype(np.float32) / 255.0, train_labels\n",
    "\n",
    "    # Load and partition dataset across processes.\n",
    "    if process_id == 0:\n",
    "        print(\"Downloading MNIST dataset...\")\n",
    "    train_images, train_labels = load_mnist()\n",
    "\n",
    "    # Partition data for distributed training.\n",
    "    samples_per_process = len(train_images) // num_processes\n",
    "    start_idx = process_id * samples_per_process\n",
    "    end_idx = start_idx + samples_per_process\n",
    "    local_images = train_images[start_idx:end_idx]\n",
    "    local_labels = train_labels[start_idx:end_idx]\n",
    "\n",
    "    print(f\"Process {process_id}: Training on samples {start_idx} to {end_idx}\")\n",
    "\n",
    "    # Create model and split into graphdef + state for functional JIT.\n",
    "    model = MLP(in_dims=784, hidden_dims=128, out_dims=10, rngs=nnx.Rngs(0))\n",
    "    graphdef, state = nnx.split(model)\n",
    "\n",
    "    # Create optimizer (functional - no stateful wrapper).\n",
    "    tx = optax.adam(learning_rate=0.001)\n",
    "    opt_state = tx.init(state)\n",
    "\n",
    "    # Pure functional training step with @jax.jit.\n",
    "    @jax.jit\n",
    "    def train_step(state, opt_state, x, y):\n",
    "        def loss_fn(state):\n",
    "            model = nnx.merge(graphdef, state)\n",
    "            logits = model(x)\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n",
    "            return loss, logits\n",
    "\n",
    "        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state)\n",
    "        updates, new_opt_state = tx.update(grads, opt_state, state)\n",
    "        new_state = optax.apply_updates(state, updates)\n",
    "        acc = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n",
    "        return loss, acc, new_state, new_opt_state\n",
    "\n",
    "    # Warmup JIT compilation before training.\n",
    "    dummy_x = jnp.zeros((1, 784))\n",
    "    dummy_y = jnp.zeros(1, dtype=jnp.int32)\n",
    "    _ = train_step(state, opt_state, dummy_x, dummy_y)\n",
    "    print(f\"Process {process_id}: JIT warmup complete\")\n",
    "\n",
    "    # Training loop.\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 5\n",
    "\n",
    "    if process_id == 0:\n",
    "        print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_start = time.perf_counter()\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Shuffle local data.\n",
    "        perm = np.random.permutation(len(local_images))\n",
    "        local_images = local_images[perm]\n",
    "        local_labels = local_labels[perm]\n",
    "\n",
    "        for i in range(0, len(local_images), BATCH_SIZE):\n",
    "            batch_x = jnp.array(local_images[i:i + BATCH_SIZE])\n",
    "            batch_y = jnp.array(local_labels[i:i + BATCH_SIZE])\n",
    "\n",
    "            loss, acc, state, opt_state = train_step(state, opt_state, batch_x, batch_y)\n",
    "            total_loss += float(loss)\n",
    "            total_acc += float(acc)\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_acc = total_acc / num_batches\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS} - Loss: {avg_loss:.4f}, Acc: {avg_acc:.4f}, Time: {time.perf_counter() - epoch_start:.2f}s\")\n",
    "\n",
    "    print(f\"Process {process_id}: Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale JAX with Kubeflow TrainJob\n",
    "\n",
    "You can use `TrainerClient()` from the Kubeflow SDK to communicate with Kubeflow Trainer APIs and scale your training function across multiple JAX training nodes.\n",
    "\n",
    "`TrainerClient()` verifies that you have required access to the Kubernetes cluster.\n",
    "\n",
    "Kubeflow Trainer creates a `TrainJob` resource and automatically sets the appropriate environment variables to set up JAX distributed training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:19:49.832393Z",
     "iopub.status.busy": "2025-09-03T13:19:49.832117Z",
     "iopub.status.idle": "2025-09-03T13:19:51.924613Z",
     "shell.execute_reply": "2025-09-03T13:19:51.924264Z",
     "shell.execute_reply.started": "2025-09-03T13:19:49.832371Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from kubeflow.trainer import CustomTrainer, TrainerClient\n",
    "\n",
    "client = TrainerClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the Training Runtimes\n",
    "\n",
    "You can get the list of available Training Runtimes to start your TrainJob.\n",
    "\n",
    "Additionally, it might show available accelerator type and number of available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:19:52.867066Z",
     "iopub.status.busy": "2025-09-03T13:19:52.866470Z",
     "iopub.status.idle": "2025-09-03T13:19:53.794000Z",
     "shell.execute_reply": "2025-09-03T13:19:53.792956Z",
     "shell.execute_reply.started": "2025-09-03T13:19:52.867027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime(name='deepspeed-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='deepspeed', image='ghcr.io/kubeflow/trainer/deepspeed-runtime:latest', num_nodes=1, device='Unknown', device_count='1'), pretrained_model=None)\n",
      "Runtime(name='jax-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='jax', image='nvcr.io/nvidia/jax:25.10-py3', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None)\n",
      "Runtime(name='mlx-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='mlx', image='ghcr.io/kubeflow/trainer/mlx-runtime:latest', num_nodes=1, device='Unknown', device_count='1'), pretrained_model=None)\n",
      "Runtime(name='torch-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', image='pytorch/pytorch:2.9.1-cuda12.8-cudnn9-runtime', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None)\n",
      "Runtime(name='torchtune-llama3.2-1b', trainer=RuntimeTrainer(trainer_type=<TrainerType.BUILTIN_TRAINER: 'BuiltinTrainer'>, framework='torchtune', image='ghcr.io/kubeflow/trainer/torchtune-trainer:latest', num_nodes=1, device='gpu', device_count='2.0'), pretrained_model=None)\n",
      "Runtime(name='torchtune-llama3.2-3b', trainer=RuntimeTrainer(trainer_type=<TrainerType.BUILTIN_TRAINER: 'BuiltinTrainer'>, framework='torchtune', image='ghcr.io/kubeflow/trainer/torchtune-trainer:latest', num_nodes=1, device='gpu', device_count='2.0'), pretrained_model=None)\n",
      "Runtime(name='torchtune-qwen2.5-1.5b', trainer=RuntimeTrainer(trainer_type=<TrainerType.BUILTIN_TRAINER: 'BuiltinTrainer'>, framework='torchtune', image='ghcr.io/kubeflow/trainer/torchtune-trainer:latest', num_nodes=1, device='gpu', device_count='2.0'), pretrained_model=None)\n"
     ]
    }
   ],
   "source": [
    "for runtime in client.list_runtimes():\n",
    "    print(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Run the Distributed TrainJob\n",
    "\n",
    "Kubeflow TrainJob will train the above model on 3 JAX nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "num_cpu=3\n",
    "num_gpu=0\n",
    "num_nodes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-03T13:19:56.525591Z",
     "iopub.status.busy": "2025-09-03T13:19:56.524936Z",
     "iopub.status.idle": "2025-09-03T13:19:56.721404Z",
     "shell.execute_reply": "2025-09-03T13:19:56.720565Z",
     "shell.execute_reply.started": "2025-09-03T13:19:56.525536Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job p87c1612ceae submitted with 3 CPU and 0 GPU\n"
     ]
    }
   ],
   "source": [
    "resources_per_node = {\n",
    "    \"cpu\": num_cpu,\n",
    "}\n",
    "if num_gpu > 0:\n",
    "    resources_per_node[\"gpu\"] = num_gpu\n",
    "\n",
    "job_name = client.train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=jax_train_mnist,\n",
    "        # Set how many JAX nodes you want to use for distributed training.\n",
    "        num_nodes=num_nodes,\n",
    "        resources_per_node=resources_per_node,\n",
    "    ),\n",
    "    runtime=\"jax-distributed\",\n",
    ")\n",
    "\n",
    "print(f\"Training job {job_name} submitted with {num_cpu} CPU and {num_gpu} GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the TrainJob steps\n",
    "\n",
    "You can check the components of TrainJob that's created.\n",
    "\n",
    "Since the TrainJob performs distributed training across 3 nodes, it generates 3 steps: `trainer-node-0` .. `trainer-node-2`.\n",
    "\n",
    "You can get the individual status for each of these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:20:01.378158Z",
     "iopub.status.busy": "2025-09-03T13:20:01.377707Z",
     "iopub.status.idle": "2025-09-03T13:20:12.713960Z",
     "shell.execute_reply": "2025-09-03T13:20:12.713295Z",
     "shell.execute_reply.started": "2025-09-03T13:20:01.378130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainJob(name='p87c1612ceae', runtime=Runtime(name='jax-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='jax', image='nvcr.io/nvidia/jax:25.10-py3', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None), steps=[Step(name='node-0', status='Running', pod_name='p87c1612ceae-node-0-0-wwtrw', device='cpu', device_count='3'), Step(name='node-1', status='Running', pod_name='p87c1612ceae-node-0-1-dscq5', device='cpu', device_count='3'), Step(name='node-2', status='Running', pod_name='p87c1612ceae-node-0-2-7vm82', device='cpu', device_count='3')], num_nodes=3, creation_timestamp=datetime.datetime(2026, 2, 6, 17, 6, 45, tzinfo=TzInfo(0)), status='Running')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for the running status.\n",
    "client.wait_for_job_status(name=job_name, status={\"Running\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:20:24.045774Z",
     "iopub.status.busy": "2025-09-03T13:20:24.045480Z",
     "iopub.status.idle": "2025-09-03T13:20:24.772877Z",
     "shell.execute_reply": "2025-09-03T13:20:24.772178Z",
     "shell.execute_reply.started": "2025-09-03T13:20:24.045755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: node-0, Status: Running, Devices: cpu x 3\n",
      "\n",
      "Step: node-1, Status: Running, Devices: cpu x 3\n",
      "\n",
      "Step: node-2, Status: Running, Devices: cpu x 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in client.get_job(name=job_name).steps:\n",
    "    print(f\"Step: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the TrainJob logs\n",
    "\n",
    "We can use the `get_job_logs()` API to get the TrainJob logs.\n",
    "\n",
    "Since we run training on 3 GPUs, every JAX node uses 60,000/3 = 20,000 images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:20:26.729486Z",
     "iopub.status.busy": "2025-09-03T13:20:26.728951Z",
     "iopub.status.idle": "2025-09-03T13:20:29.596510Z",
     "shell.execute_reply": "2025-09-03T13:20:29.594741Z",
     "shell.execute_reply.started": "2025-09-03T13:20:26.729446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Distributed JAX env on node-0 **\n",
      "=====================================\n",
      "ERROR:2026-02-06 17:06:48,122:jax._src.xla_bridge:487: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda13.initialize()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jax/jax/_src/xla_bridge.py\", line 485, in discover_pjrt_plugins\n",
      "    plugin_module.initialize()\n",
      "  File \"/opt/jaxlibs/jax_cuda13_pjrt/jax_plugins/xla_cuda13/__init__.py\", line 328, in initialize\n",
      "    _check_cuda_versions(raise_on_first_error=True)\n",
      "  File \"/opt/jaxlibs/jax_cuda13_pjrt/jax_plugins/xla_cuda13/__init__.py\", line 285, in _check_cuda_versions\n",
      "    local_device_count = cuda_versions.cuda_device_count()\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: jaxlib/cuda/versions_helpers.cc:113: operation cuInit(0) failed: Unknown CUDA error 303; cuGetErrorName failed. This probably means that JAX was unable to load the CUDA libraries.\n",
      "JAX distributed initialized: Process 0/3\n",
      "Available devices: [CpuDevice(id=0), CpuDevice(id=2048), CpuDevice(id=4096)]\n",
      "Downloading MNIST dataset...\n",
      "Process 0: Training on samples 0 to 20000\n",
      "Process 0: JIT warmup complete\n",
      "Starting training for 5 epochs...\n",
      "Epoch 1/5 - Loss: 0.5693, Acc: 0.8465, Time: 0.75s\n",
      "Epoch 2/5 - Loss: 0.2212, Acc: 0.9373, Time: 0.27s\n",
      "Epoch 3/5 - Loss: 0.1595, Acc: 0.9542, Time: 0.24s\n",
      "Epoch 4/5 - Loss: 0.1247, Acc: 0.9636, Time: 0.37s\n",
      "Epoch 5/5 - Loss: 0.0964, Acc: 0.9715, Time: 0.25s\n",
      "Process 0: Training complete!\n",
      "\n",
      "** Distributed JAX env on node-1 **\n",
      "=====================================\n",
      "ERROR:2026-02-06 17:06:48,122:jax._src.xla_bridge:487: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda13.initialize()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jax/jax/_src/xla_bridge.py\", line 485, in discover_pjrt_plugins\n",
      "    plugin_module.initialize()\n",
      "  File \"/opt/jaxlibs/jax_cuda13_pjrt/jax_plugins/xla_cuda13/__init__.py\", line 328, in initialize\n",
      "    _check_cuda_versions(raise_on_first_error=True)\n",
      "  File \"/opt/jaxlibs/jax_cuda13_pjrt/jax_plugins/xla_cuda13/__init__.py\", line 285, in _check_cuda_versions\n",
      "    local_device_count = cuda_versions.cuda_device_count()\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: jaxlib/cuda/versions_helpers.cc:113: operation cuInit(0) failed: Unknown CUDA error 303; cuGetErrorName failed. This probably means that JAX was unable to load the CUDA libraries.\n",
      "JAX distributed initialized: Process 1/3\n",
      "Available devices: [CpuDevice(id=0), CpuDevice(id=2048), CpuDevice(id=4096)]\n",
      "Process 1: Training on samples 20000 to 40000\n",
      "Process 1: JIT warmup complete\n",
      "Epoch 1/5 - Loss: 0.5728, Acc: 0.8481, Time: 0.74s\n",
      "Epoch 2/5 - Loss: 0.2320, Acc: 0.9317, Time: 0.27s\n",
      "Epoch 3/5 - Loss: 0.1692, Acc: 0.9501, Time: 0.26s\n",
      "Epoch 4/5 - Loss: 0.1324, Acc: 0.9607, Time: 0.35s\n",
      "Epoch 5/5 - Loss: 0.1042, Acc: 0.9690, Time: 0.25s\n",
      "Process 1: Training complete!\n",
      "\n",
      "** Distributed JAX env on node-2 **\n",
      "=====================================\n",
      "ERROR:2026-02-06 17:06:48,122:jax._src.xla_bridge:487: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda13.initialize()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jax/jax/_src/xla_bridge.py\", line 485, in discover_pjrt_plugins\n",
      "    plugin_module.initialize()\n",
      "  File \"/opt/jaxlibs/jax_cuda13_pjrt/jax_plugins/xla_cuda13/__init__.py\", line 328, in initialize\n",
      "    _check_cuda_versions(raise_on_first_error=True)\n",
      "  File \"/opt/jaxlibs/jax_cuda13_pjrt/jax_plugins/xla_cuda13/__init__.py\", line 285, in _check_cuda_versions\n",
      "    local_device_count = cuda_versions.cuda_device_count()\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: jaxlib/cuda/versions_helpers.cc:113: operation cuInit(0) failed: Unknown CUDA error 303; cuGetErrorName failed. This probably means that JAX was unable to load the CUDA libraries.\n",
      "JAX distributed initialized: Process 2/3\n",
      "Available devices: [CpuDevice(id=0), CpuDevice(id=2048), CpuDevice(id=4096)]\n",
      "Process 2: Training on samples 40000 to 60000\n",
      "Process 2: JIT warmup complete\n",
      "Epoch 1/5 - Loss: 0.5635, Acc: 0.8516, Time: 0.81s\n",
      "Epoch 2/5 - Loss: 0.2313, Acc: 0.9320, Time: 0.37s\n",
      "Epoch 3/5 - Loss: 0.1677, Acc: 0.9519, Time: 0.25s\n",
      "Epoch 4/5 - Loss: 0.1382, Acc: 0.9597, Time: 0.19s\n",
      "Epoch 5/5 - Loss: 0.1084, Acc: 0.9679, Time: 0.16s\n",
      "Process 2: Training complete!\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"\\n** Distributed JAX env on node-{i} **\")\n",
    "    print(f\"=====================================\")\n",
    "    print(\"\\n\".join(TrainerClient().get_job_logs(name=job_name, follow=True, step=f\"node-{i}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the TrainJob\n",
    "\n",
    "When TrainJob is finished, you can delete the resource.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.delete_job(job_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
